{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "import datasets\n",
    "import transformers\n",
    "from datasets import load_dataset,load_from_disk\n",
    "from evaluate import load\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader \n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "import argparse\n",
    "from peft import PeftModel    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# \n",
    "# MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "def add_args(parser: argparse.ArgumentParser):\n",
    "\n",
    "    parser.add_argument('--checkpoint_dir',\n",
    "                            type=str,\n",
    "                            default='experiments/Mistral-7B-v0.1/checkpoint-1000')\n",
    "\n",
    "    parser.add_argument('--model_name',\n",
    "                            type=str,\n",
    "                            default='mistralai/Mistral-7B-v0.1')\n",
    "\n",
    "    parser.add_argument('--save_file',\n",
    "                                type=str,\n",
    "                                default='pred_output.txt')\n",
    "    \n",
    "    parser.add_argument('--batch_size',\n",
    "                            type=int,\n",
    "                            default=32)\n",
    "    \n",
    "    parser.add_argument('--prompt',\n",
    "                            type=str,\n",
    "                            default=\"\"\"\n",
    "Below is a clue for a decrypting crossword. Your task is to solve this clue. The number of charachters in the answer should be same as the number in the parenthesis. Just output the answer only. Do not output any explanitions, just the words in the answer.\n",
    " \n",
    "### Input:\n",
    "Desk register taken no further than Ozzie? (7)\n",
    "\n",
    "### Output:\n",
    "rolltop\n",
    "\n",
    "### Input:\n",
    "Henry has books stolen (3)\n",
    "\n",
    "### Output:\n",
    "hot\n",
    "\"\"\")\n",
    "    \n",
    "    parser.add_argument('--n_shots',\n",
    "                            type=int,\n",
    "                            default=0)\n",
    "    \n",
    "    parser.add_argument('--num_examples',\n",
    "                            type=int,\n",
    "                            default=100)\n",
    "    parser.add_argument('--dataset_path',\n",
    "                            type=str,\n",
    "                            default='data/unique_targets')\n",
    "    parser.add_argument('--dataset_type',\n",
    "                            type=int,\n",
    "                            default=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def concat_length(example):\n",
    "\n",
    "    example[\"clue\"] = f'{example[\"clue\"]} ({example[\"orig_lengths\"]})'\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "# DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "# Below is a clue for a decrypting crossword. Your task is to solve this clue. The number of charachters in the answer should be same as the number in the parenthesis. Just output the answer only. Do not output any explanitions, just the words in the answer.\n",
    " \n",
    "# ### Input:\n",
    "# Desk register taken no further than Ozzie? (7)\n",
    "\n",
    "# ### Output:\n",
    "# rolltop\n",
    "\n",
    "# ### Input:\n",
    "# Henry has books stolen (3)\n",
    "\n",
    "# ### Output:\n",
    "# hot\n",
    "# \"\"\".strip()\n",
    "\n",
    "\n",
    "# def generate_training_prompt(\n",
    "#     clue: str, prompt: str = DEFAULT_SYSTEM_PROMPT\n",
    "# ) -> str:\n",
    "    \n",
    "\n",
    "#     return f\"\"\"### Instruction: {prompt}\n",
    "\n",
    "# ### Input:\n",
    "# {clue.strip()}\n",
    "\n",
    "# \"\"\".strip()\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def map_prompt(ex, base_prompt, shots):\n",
    "\n",
    "\n",
    "    p = ''\n",
    "\n",
    "    #add base prompt\n",
    "    p = f'### Instruction: {base_prompt}\\n\\n'\n",
    "\n",
    "    for shot in shots:\n",
    "        p += f'### Input:\\n{shot[\"clue\"]}\\n\\n### Output:\\n{shot[\"soln_with_spaces\"]}\\n\\n'\n",
    "\n",
    "\n",
    "    p+= f'### Input:\\n{ex[\"clue\"]}'\n",
    "\n",
    "\n",
    "    ex['prompt'] = p\n",
    "    return ex\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def inference(prompts, tokenizer, generation_config, model):\n",
    "    \n",
    "   \n",
    "    encoding = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "\n",
    "    answer_lengthes = []\n",
    "\n",
    "    for t in prompts:\n",
    "        l = t.split('\\n')[-1]\n",
    "        answer_lengthes. append( l[l.rfind(\"(\")+1:l.rfind(\")\")].split(',')) \n",
    "\n",
    "    answer_lengthes =  [ list(map(int, answer_lengthes[i]))  for i in range(len(answer_lengthes))] \n",
    "\n",
    "    # print(answer_lengthes)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **encoding,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=True,\n",
    "            temperature=0.00001,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            generation_config=generation_config,\n",
    "        )  \n",
    "\n",
    "    answer_tokens = outputs[:, encoding.input_ids.shape[1] :]\n",
    "    output_text = tokenizer.batch_decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "    \n",
    "\n",
    "    return output_text, answer_lengthes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_dir experiments/Mistral-7B-v0.1/checkpoint-1000\n",
      "model_name mistralai/Mistral-7B-v0.1\n",
      "save_file pred_output.txt\n",
      "batch_size 32\n",
      "prompt \n",
      "Below is a clue for a decrypting crossword. Your task is to solve this clue. The number of charachters in the answer should be same as the number in the parenthesis. Just output the answer only. Do not output any explanitions, just the words in the answer.\n",
      " \n",
      "### Input:\n",
      "Desk register taken no further than Ozzie? (7)\n",
      "\n",
      "### Output:\n",
      "rolltop\n",
      "\n",
      "### Input:\n",
      "Henry has books stolen (3)\n",
      "\n",
      "### Output:\n",
      "hot\n",
      "\n",
      "n_shots 0\n",
      "num_examples 100\n",
      "dataset_path data/unique_targets\n",
      "dataset_type 0\n",
      "Dataset({\n",
      "    features: ['labels', 'clue'],\n",
      "    num_rows: 5629\n",
      "})\n",
      " total number of examples: 5629,    number of unique answers: 5629\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614fcf1f19884f288f5af912098450a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "    \n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser('Eval LLMs on crossword solving')\n",
    "\n",
    "add_args(parser)\n",
    "args, _ = parser.parse_known_args()\n",
    "# MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "for arg in vars(args):\n",
    "    print(arg, getattr(args, arg))\n",
    "\n",
    "MODEL_NAME = args.model_name\n",
    "batch_size = args.batch_size\n",
    "prompt = args.prompt\n",
    "num_examples = args.num_examples\n",
    "save_file = args.save_file\n",
    "\n",
    "dataset_path = args.dataset_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# val_dataset = load_dataset('json', data_files=dataset_path, field=\"val\",split=\"train\")\n",
    "\n",
    "if args.dataset_type:\n",
    "    val_dataset = load_dataset('json', data_files=dataset_path, field=\"train\",split=\"train\")\n",
    "    val_dataset = val_dataset.map(concat_length)\n",
    "    unique_answers = np.unique(val_dataset['soln'])\n",
    "    val_dataset = val_dataset.select_columns(['soln_with_spaces', 'clue' ])\n",
    "\n",
    "\n",
    "else:\n",
    "    val_dataset = load_from_disk(dataset_path)\n",
    "    val_dataset = val_dataset['test']\n",
    "    unique_answers = np.unique(val_dataset['labels'])\n",
    "    print(val_dataset)\n",
    "\n",
    "    val_dataset = val_dataset.rename_column('labels', 'soln_with_spaces')\n",
    "\n",
    "\n",
    "idx= np.random.randint(0,len(val_dataset),args.n_shots)\n",
    "\n",
    "shots = val_dataset.select(idx)\n",
    "\n",
    "for shot in shots:\n",
    "    print(shot['clue'], shot['soln_with_spaces'])\n",
    "\n",
    "    \n",
    "val_dataset = val_dataset.map(map_prompt,fn_kwargs={\"base_prompt\": prompt,\"shots\":shots})\n",
    "\n",
    "\n",
    "\n",
    "print(f' total number of examples: {len(val_dataset)},    number of unique answers: {len(unique_answers)}')\n",
    "\n",
    "if num_examples == 0:\n",
    "    num_examples = len(val_dataset)\n",
    "\n",
    "\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset.select(range(num_examples)),batch_size = batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    return_dict=True,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "if args.checkpoint_dir:\n",
    "    adapter_checkpoint  = args.checkpoint_dir\n",
    "    model = PeftModel.from_pretrained(model, adapter_checkpoint)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "acc_metric = load(\"accuracy\")\n",
    "\n",
    "\n",
    "model = model.eval()\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define PAD Token = BOS Token\n",
    "tokenizer.pad_token = tokenizer.bos_token\n",
    "model.config.pad_token_id = model.config.bos_token_id\n",
    "\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "original_predictions = []\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:11<00:00, 17.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 300\n",
      "Number of Examples 100\n",
      "ACCURACY:  0.11\n",
      "Length error:  0.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for batch in tqdm(val_dataloader):\n",
    "\n",
    "    prompts = batch['prompt']\n",
    "\n",
    "    # for x in prompts:\n",
    "    #     print(x)   \n",
    "    # break\n",
    "\n",
    "    # labels.extend (batch['soln_with_spaces'])\n",
    "    ans = []\n",
    "\n",
    "    output_text, answer_lengths = inference(prompts=prompts, tokenizer=tokenizer, generation_config=generation_config, model=model)\n",
    "    \n",
    "\n",
    "\n",
    "    # print(output_text)\n",
    "    # break\n",
    "    for i,t in enumerate(output_text):\n",
    "\n",
    "        lines = t.split('\\n')\n",
    "        for j,l in enumerate(lines):\n",
    "            if l=='### Response:' or l=='### Output:':\n",
    "                labels.append( batch['soln_with_spaces'][i].lower())\n",
    "\n",
    "                ## Cut the answer to the length of the answer given in the clue\n",
    "                answer = []\n",
    "                original_words = lines[j+1].lower().split(' ')\n",
    "                if len(original_words) >= len(answer_lengths[i]):\n",
    "                    for idx, length in enumerate(answer_lengths[i]):\n",
    "\n",
    "                        answer.append(original_words[idx][:length])\n",
    "\n",
    "\n",
    "                    predictions.append(' '.join(answer))\n",
    "                else:\n",
    "                    predictions.append(lines[j+1].lower())\n",
    "\n",
    "                original_predictions.append(lines[j+1].lower())\n",
    "\n",
    "\n",
    "                break\n",
    "        # print( answer_lengths[i])\n",
    "        \n",
    "        # print(output_text)\n",
    "        # break\n",
    "\n",
    "print(len(predictions), len(labels))\n",
    "assert (len(predictions) == len(labels))\n",
    "\n",
    "\n",
    "correct = 0\n",
    "length_error =0\n",
    "\n",
    "\n",
    "\n",
    "save_file = 'outputs/' + 'mistral_1k_unique.txt'\n",
    "with open(save_file, 'w') as f:\n",
    "    for original,pred,label in zip(original_predictions,predictions,labels):\n",
    "    # for pred,label in zip(predictions,labels):\n",
    "\n",
    "        pred  = \" \".join(pred.split())\n",
    "        label = \" \".join(label.split())\n",
    "\n",
    "        correctly_predicted = False\n",
    "        if pred == label:\n",
    "            correct +=1\n",
    "            correctly_predicted = True\n",
    "\n",
    "        if len(pred) != len(label):\n",
    "            length_error +=1\n",
    "\n",
    "        f.write(f'Original output: {original}\\n')\n",
    "        if correctly_predicted:\n",
    "            f.write(emoji.emojize(f'{pred} | {label}  :check_mark_button: \\n'))\n",
    "        else:\n",
    "            f.write(emoji.emojize(f'{pred} | {label}  :cross_mark: \\n'))\n",
    "\n",
    "        f.write('---------------------------------------------------------------------------------- \\n\\n')\n",
    "    f.seek(0)\n",
    "    f.write(f'Dataset: {args.dataset_path}\\n')\n",
    "    f.write(f'Number of Examples {num_examples}\\n')\n",
    "    print(f'Number of Examples {num_examples}')\n",
    "    f.write(f'ACCURACY:  { float (correct / num_examples)}\\n')\n",
    "    print(f'ACCURACY:  { float (correct / num_examples)}')\n",
    "    f.write(f'Length error:  { float ((length_error / num_examples) )}\\n')\n",
    "    print(f'Length error:  { float ((length_error / num_examples) )}')\n",
    "    f.write('----------------------------------------------------- \\n\\n')\n",
    "\n",
    "    # for output in output_text:\n",
    "    #     f.write(output)\n",
    "    #     f.write('\\n\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
