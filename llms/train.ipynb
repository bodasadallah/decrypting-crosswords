{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from evaluate import load\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from args_parser import get_args\n",
    "import re\n",
    "from pathlib import Path\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging_dir ./logs\n",
      "use_flash_attention_2 False\n",
      "report_to tensorboard\n",
      "max_steps 10000\n",
      "save_steps 400\n",
      "logging_steps 100\n",
      "max_seq_length 256\n",
      "checkpoint_path None\n",
      "do_eval False\n",
      "do_train False\n",
      "evaluation_strategy steps\n",
      "eval_steps 10\n",
      "log_level info\n",
      "logging_strategy steps\n",
      "save_total_limit 10\n",
      "run_name LLama2\n",
      "base_prompt Below is a clue for a decrypting crossword. Your task is to solve this clue. The number of characters in the answer should be same as the number in the parenthesis. Just output the answer only.\n",
      "dataset_path ../data/disjoint_word_init.json\n",
      "field prompt\n",
      "model_name meta-llama/Llama-2-7b-hf\n",
      "output_dir ./experiments\n",
      "per_device_train_batch_size 4\n",
      "per_device_val_batch_size 2\n",
      "gradient_accumulation_steps 2\n",
      "optim paged_adamw_32bit\n",
      "learning_rate 0.0002\n",
      "max_grad_norm 0.3\n",
      "warmup_ratio 0.03\n",
      "lr_scheduler_type constant\n",
      "group_by_length True\n",
      "bnb_4bit_quant_type nf4\n",
      "bnb_4bit_compute_dtype bfloat16\n",
      "bnb_4bit_use_double_quant True\n",
      "gradient_checkpointing False\n",
      "lora_alpha 16\n",
      "lora_dropout 0.1\n",
      "lora_r 64\n",
      "bias none\n",
      "task_type CAUSAL_LM\n",
      "lora_target_modules ['q_proj', 'up_proj', 'o_proj', 'k_proj', 'down_proj', 'gate_proj', 'v_proj']\n"
     ]
    }
   ],
   "source": [
    "args = get_args()\n",
    "\n",
    "args.field = 'prompt'\n",
    "\n",
    "for arg in vars(args):\n",
    "    print(arg, getattr(args, arg))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d541b53594340faa419289821e7a0d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 262410240 || all params: 3500412928 || trainable%: 7.496550989769399\n",
      "save_steps: 400\n",
      "logging_steps: 100\n",
      "Saving the model to ./experiments/Llama-2-7b-hf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = args.model_name\n",
    "\n",
    "## Bits and Bytes config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    # bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    use_flash_attention_2=args.use_flash_attention_2,\n",
    ")\n",
    "\n",
    "\n",
    "## Enable gradient checkpointing\n",
    "if args.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "## Prepare model for k-bit training\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "\n",
    "## Print the number of trainable parameters\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "## Silence the warnings\n",
    "model.config.use_cache = False\n",
    "\n",
    "## Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "\n",
    "output_dir = args.output_dir\n",
    "\n",
    "per_device_train_batch_size =  args.per_device_train_batch_size\n",
    "per_device_val_batch_size = args.per_device_val_batch_size\n",
    "gradient_accumulation_steps = args.gradient_accumulation_steps\n",
    "\n",
    "\n",
    "\n",
    "optim = args.optim\n",
    "\n",
    "save_steps = args.save_steps\n",
    "logging_steps = args.logging_steps\n",
    "learning_rate = args.learning_rate\n",
    "max_grad_norm = args.max_grad_norm\n",
    "\n",
    "\n",
    "\n",
    "print(f\"save_steps: {save_steps}\")\n",
    "print(f\"logging_steps: {logging_steps}\")\n",
    "\n",
    "\n",
    "\n",
    "warmup_ratio = args.warmup_ratio\n",
    "lr_scheduler_type = args.lr_scheduler_type\n",
    "\n",
    "\n",
    "output_dir = args.output_dir + f\"/{model_name.split('/')[-1]}\"\n",
    "loggig_dir = args.logging_dir + f\"/{model_name.split('/')[-1]}\" + f\"/logs\"\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(loggig_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Saving the model to {output_dir}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lora_alpha = args.lora_alpha\n",
    "lora_dropout = args.lora_dropout\n",
    "lora_r = args.lora_r\n",
    "lora_target_modules = args.lora_target_modules\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules = lora_target_modules\n",
    ")\n",
    "\n",
    "\n",
    "max_seq_length = args.max_seq_length\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the datasets\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading the datasets\")\n",
    "train_dataset   = get_dataset(dataset_path=args.dataset_path,tokenizer=tokenizer, field = args.field, split = 'train')\n",
    "val_dataset     = get_dataset(dataset_path=args.dataset_path,tokenizer=tokenizer, field = args.field, split = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_output(batch):\n",
    "    \n",
    "    outputs = []\n",
    "    for j,sample in enumerate(batch):\n",
    "        lines = sample.split('\\n')\n",
    "        for i,l in enumerate(lines):\n",
    "\n",
    "            if l=='### Response:':\n",
    "                outputs.append( lines[i+1].lower().strip() )\n",
    "                break\n",
    "        if len(outputs) <= j :\n",
    "            outputs.append('')\n",
    "    \n",
    "    return outputs\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokenizer_test = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# tokenizer_test.pad_token = tokenizer.eos_token\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels, inputs = eval_pred\n",
    "    # predictions = predictions[:, 0]\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    pred = predictions.argmax(-1)\n",
    "    \n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    pred = tokenizer.batch_decode(pred, skip_special_tokens=True)\n",
    "\n",
    "    extracted_labels = extract_output(labels)\n",
    "    extracted_pred = extract_output(pred)\n",
    "    \n",
    "\n",
    "    correct = 0\n",
    "    for l,p in zip(extracted_labels, extracted_pred):\n",
    "        correct += int(l==p)\n",
    "\n",
    "    return { 'accuracy': correct/len(labels) }\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    per_device_eval_batch_size=per_device_val_batch_size,\n",
    "    evaluation_strategy=args.evaluation_strategy,\n",
    "    eval_steps=10,\n",
    "    logging_steps=10,\n",
    "    run_name=args.run_name,\n",
    "    save_steps=save_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=100,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=args.report_to,\n",
    "    gradient_checkpointing=args.gradient_checkpointing,\n",
    "    neftune_noise_alpha=0.1,\n",
    "    logging_dir=loggig_dir,\n",
    "    include_inputs_for_metrics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'clue', 'labels'],\n",
       "    num_rows: 32628\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = val_dataset.select(range(10))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Instruction: Below is a clue for a decrypting crossword. Your task is to solve this clue. The number of characters in the answer should be same as the number in the parenthesis. Just output the answer only.\\n\\n### Input:\\nTension in an arm? Slightly (1,6)\\n\\n### Response:\\na trifle'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0]['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=args.field,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3eb17ef47b54b0ba7af0c356dce7bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8151, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16cf5c0b1bba40748dc62eb94b60dafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8718175888061523, 'eval_accuracy': 0.0, 'eval_runtime': 171.797, 'eval_samples_per_second': 0.058, 'eval_steps_per_second': 0.029, 'epoch': 0.0}\n",
      "{'loss': 0.8515, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9698f28274774c3789904d4cfd3c5a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7333288788795471, 'eval_accuracy': 0.0, 'eval_runtime': 1.64, 'eval_samples_per_second': 6.097, 'eval_steps_per_second': 3.049, 'epoch': 0.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     trainer\u001b[39m.\u001b[39mtrain(resume_from_checkpoint\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mcheckpoint_path)\n\u001b[1;32m      7\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     11\u001b[0m \u001b[39m# trainer.save_model()\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone training\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:1556\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1554\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1555\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1556\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1557\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1558\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1559\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1560\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1561\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:1861\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1858\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1860\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1861\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1863\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1864\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1865\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1866\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1867\u001b[0m ):\n\u001b[1;32m   1868\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py:2735\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2733\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m   2734\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2735\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[1;32m   2737\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/accelerate/accelerator.py:1987\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1985\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1986\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1987\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1988\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1989\u001b[0m     loss\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)\n",
    "\n",
    "if args.checkpoint_path:\n",
    "    trainer.train(resume_from_checkpoint=args.checkpoint_path)\n",
    "else:\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "# trainer.save_model()\n",
    "\n",
    "\n",
    "print(\"Done training\")\n",
    "print(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
