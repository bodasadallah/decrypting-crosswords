starting Evaluation.......................
<class 'dict'>
------------------Loading boda/guardian_naive_random/test, and using LLAMA3_BASE_PROMPT, and 0 shot prompt ------------------
 Dataset sample: {'input': 'Achy shaking stopped by iodine, salt and kaolin (5,4)', 'target': 'china clay', 'prompt': 'You are a cryptic crossword expert. You are given a clue for a cryptic crossword. Output only the answer. \nclue:\nAchy shaking stopped by iodine, salt and kaolin (5,4)\noutput:\n'}
WARNING 07-28 09:29:51 utils.py:569] Gemma 2 uses sliding window attention for every odd layer, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).
INFO 07-28 09:29:51 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='google/gemma-2-9b-it', speculative_config=None, tokenizer='google/gemma-2-9b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=google/gemma-2-9b-it, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 07-28 09:29:52 selector.py:80] Using Flashinfer backend.
INFO 07-28 09:29:52 model_runner.py:680] Starting to load model google/gemma-2-9b-it...
INFO 07-28 09:29:52 selector.py:80] Using Flashinfer backend.
INFO 07-28 09:29:53 weight_utils.py:223] Using model weights format ['*.safetensors']
INFO 07-28 09:30:06 model_runner.py:692] Loading model weights took 17.3781 GB
INFO 07-28 09:30:07 gpu_executor.py:102] # GPU blocks: 2990, # CPU blocks: 780
INFO 07-28 09:30:10 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 07-28 09:30:10 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 07-28 09:30:25 model_runner.py:1181] Graph capturing finished in 15 secs.
len(raw_predictions): 28476, len cleaned_predictions: 890
Predictions saved to results/gemma-2-9b-it-guardian_naive_random-base/predictions.json
Running Evaluation
Number of Examples 28476

Cleaned ACCURACY:  0.04863744907992695

Orginal ACCURACY:  0.04863744907992695

Cleaned Length error:  0.523598820058997

Original Length error:  0.5236339373507515

----------------------------------------------------- 


 ending 
