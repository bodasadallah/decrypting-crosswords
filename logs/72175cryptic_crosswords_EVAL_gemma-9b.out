starting Evaluation.......................
<class 'dict'>
------------------Loading boda/guardian_naive_random/test, and using ALL_INCLUSIVE_PROMPT, and 0 shot prompt ------------------
 Dataset sample: {'input': 'Achy shaking stopped by iodine, salt and kaolin (5,4)', 'target': 'china clay', 'prompt': 'You are a cryptic crossword expert. The cryptic clue consists of a definition and a wordplay.\nThe definition is a synonym of the answer and usually comes at the beginning or the end of the clue.\nThe wordplay gives some instructions on how to get to the answer in another (less literal) way.\nThe number/s in the parentheses at the end of the clue indicates the number of letters in the answer.\nExtract the definiton and the wordplay in the clue, and use them to solve the clue. Finally, output the answer on this format:\nAnswer: <answer>,\nClue:\nAchy shaking stopped by iodine, salt and kaolin (5,4)\n'}
WARNING 07-28 09:29:38 utils.py:569] Gemma 2 uses sliding window attention for every odd layer, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).
INFO 07-28 09:29:38 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='google/gemma-2-9b-it', speculative_config=None, tokenizer='google/gemma-2-9b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=google/gemma-2-9b-it, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 07-28 09:29:39 selector.py:80] Using Flashinfer backend.
INFO 07-28 09:29:40 model_runner.py:680] Starting to load model google/gemma-2-9b-it...
INFO 07-28 09:29:40 selector.py:80] Using Flashinfer backend.
INFO 07-28 09:29:40 weight_utils.py:223] Using model weights format ['*.safetensors']
INFO 07-28 09:29:53 model_runner.py:692] Loading model weights took 17.3781 GB
INFO 07-28 09:29:54 gpu_executor.py:102] # GPU blocks: 2990, # CPU blocks: 780
INFO 07-28 09:29:57 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 07-28 09:29:57 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 07-28 09:30:15 model_runner.py:1181] Graph capturing finished in 17 secs.
len(raw_predictions): 28476, len cleaned_predictions: 890
Predictions saved to results/gemma-2-9b-it-guardian_naive_random-all_inclusive/predictions.json
Running Evaluation
Number of Examples 28476

Cleaned ACCURACY:  0.02423093131057733

Orginal ACCURACY:  0.02423093131057733

Cleaned Length error:  0.7786205927798848

Original Length error:  0.7785152409046214

----------------------------------------------------- 


 ending 
