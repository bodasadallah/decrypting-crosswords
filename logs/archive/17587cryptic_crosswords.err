2024-02-11 20:34:51.506609: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-02-11 20:34:51.506758: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-02-11 20:34:51.507514: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-02-11 20:34:51.512169: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-11 20:34:52.349636: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.49s/it]
Map:   0%|          | 0/32628 [00:00<?, ? examples/s]Map:  12%|█▏        | 4000/32628 [00:00<00:01, 24517.70 examples/s]Map:  25%|██▍       | 8000/32628 [00:00<00:00, 24937.61 examples/s]Map:  37%|███▋      | 12000/32628 [00:00<00:00, 25084.63 examples/s]Map:  49%|████▉     | 16000/32628 [00:00<00:00, 25362.91 examples/s]Map:  61%|██████▏   | 20000/32628 [00:00<00:00, 25433.78 examples/s]Map:  74%|███████▎  | 24000/32628 [00:00<00:00, 25535.60 examples/s]Map:  86%|████████▌ | 28000/32628 [00:01<00:00, 25647.95 examples/s]Map:  98%|█████████▊| 32000/32628 [00:01<00:00, 25645.62 examples/s]Map: 100%|██████████| 32628/32628 [00:01<00:00, 23399.46 examples/s]
wandb: Currently logged in as: bodasadallah2. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home/abdelrahman.sadallah/mbzuai/decrypting-crosswords/llms/wandb/run-20240211_203645-e0pq9jon
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistralai/Mistral-7B-v0.1
wandb: ⭐️ View project at https://wandb.ai/bodasadallah2/huggingface
wandb: 🚀 View run at https://wandb.ai/bodasadallah2/huggingface/runs/e0pq9jon
  0%|          | 0/3000 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.
  0%|          | 1/3000 [00:10<9:07:48, 10.96s/it]                                                    0%|          | 1/3000 [00:10<9:07:48, 10.96s/it]  0%|          | 2/3000 [00:18<7:41:04,  9.23s/it]  0%|          | 3/3000 [00:26<7:09:51,  8.61s/it]  0%|          | 4/3000 [00:34<6:52:27,  8.26s/it]  0%|          | 5/3000 [00:42<6:40:46,  8.03s/it]  0%|          | 6/3000 [00:49<6:31:42,  7.85s/it]  0%|          | 7/3000 [00:57<6:25:41,  7.73s/it]  0%|          | 8/3000 [01:04<6:19:14,  7.61s/it]  0%|          | 9/3000 [01:11<6:15:55,  7.54s/it]  0%|          | 10/3000 [01:19<6:11:53,  7.46s/it]  0%|          | 11/3000 [01:26<6:09:05,  7.41s/it]  0%|          | 12/3000 [01:33<6:07:17,  7.38s/it]  0%|          | 13/3000 [01:40<6:04:12,  7.32s/it]  0%|          | 14/3000 [01:48<6:02:01,  7.27s/it]  0%|          | 15/3000 [01:55<6:01:26,  7.27s/it]  1%|          | 16/3000 [02:02<5:59:17,  7.22s/it]  1%|          | 17/3000 [02:09<5:57:45,  7.20s/it]  1%|          | 18/3000 [02:16<5:56:38,  7.18s/it]  1%|          | 19/3000 [02:23<5:56:47,  7.18s/it]  1%|          | 20/3000 [02:31<5:55:11,  7.15s/it]  1%|          | 21/3000 [02:38<5:54:01,  7.13s/it]  1%|          | 22/3000 [02:45<5:53:12,  7.12s/it]  1%|          | 23/3000 [02:52<5:52:34,  7.11s/it]  1%|          | 24/3000 [02:59<5:52:44,  7.11s/it]  1%|          | 25/3000 [03:06<5:51:27,  7.09s/it]  1%|          | 26/3000 [03:13<5:50:31,  7.07s/it]  1%|          | 27/3000 [03:20<5:49:49,  7.06s/it]  1%|          | 28/3000 [03:27<5:49:19,  7.05s/it]