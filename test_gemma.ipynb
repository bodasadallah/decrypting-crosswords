{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import argparse\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from inference import llama3_inference\n",
    "import re\n",
    "from calc_scores import calc_and_save_acc\n",
    "from utils import crop_predictions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader \n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "import argparse\n",
    "from  prompts import PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_llama(prompt, clue, target, model, tokenizer, ans='',  definition = ' '):\n",
    "    correct = 0\n",
    "    \n",
    "    messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt.format(clue=clue, ans=ans, definition=definition)},\n",
    "]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "    \n",
    "    response = llama3_inference (model, tokenizer, [prompt],do_sample= False,temp=0.1, top_p=0.1 )[0].lower()\n",
    "    # response = model.generate([prompt], sampling_params, use_tqdm= False)[0].outputs[0].text.strip().lower()\n",
    "\n",
    "    # print(response)\n",
    "\n",
    "    for l in response.split('\\n'):\n",
    "            if 'answer:' in l:\n",
    "                response = l.split('answer:')[1].strip().replace(',','').replace('.','').replace('?','').replace('!','').replace('*','').strip()\n",
    "\n",
    "    if re.findall('\"([^\"]*)\"', response):\n",
    "        response = re.findall('\"([^\"]*)\"', response)[0]\n",
    "    \n",
    "    elif 'definition word' in response:\n",
    "        response = response.split('definition word is:')[1]\n",
    "    elif 'wordplay type is' in response:\n",
    "        response = response.split('wordplay type is:')[1]\n",
    "    \n",
    "\n",
    "\n",
    "    # response = response.split('\\n')[0]\n",
    "    for d in target:\n",
    "        if str(d).strip() == response.strip():\n",
    "            correct = 1\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return correct, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-28 08:30:53 utils.py:569] Gemma 2 uses sliding window attention for every odd layer, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).\n",
      "INFO 07-28 08:30:53 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='google/gemma-2-9b-it', speculative_config=None, tokenizer='google/gemma-2-9b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=google/gemma-2-9b-it, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 07-28 08:30:55 selector.py:80] Using Flashinfer backend.\n",
      "INFO 07-28 08:30:56 model_runner.py:680] Starting to load model google/gemma-2-9b-it...\n",
      "INFO 07-28 08:30:56 selector.py:80] Using Flashinfer backend.\n",
      "INFO 07-28 08:30:57 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b6bb6e065c41a0bf5f2e539092ae6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-28 08:31:13 model_runner.py:692] Loading model weights took 17.3781 GB\n",
      "INFO 07-28 08:31:14 gpu_executor.py:102] # GPU blocks: 2990, # CPU blocks: 780\n",
      "INFO 07-28 08:31:18 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 07-28 08:31:18 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-28 08:31:38 model_runner.py:1181] Graph capturing finished in 19 secs.\n"
     ]
    }
   ],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     'google/gemma-2-9b-it',\n",
    "#     attn_implementation= \"flash_attention_2\",\n",
    "#     torch_dtype = torch.bfloat16,\n",
    "#     device_map = 'auto'\n",
    "# )\n",
    "# model = model.eval()\n",
    "# tokenizer = AutoTokenizer.from_pretrained('google/gemma-2-9b-it')\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# model.config.pad_token_id = model.config.bos_token_id\n",
    "\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"VLLM_ATTENTION_BACKEND\"] = \"FLASHINFER\"\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "model = LLM(\n",
    "    model=\"google/gemma-2-9b-it\",\n",
    "    # gpu_memory_utilization=0.9,\n",
    "    max_model_len=256\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = model.get_tokenizer()\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.0, top_p=1, max_tokens=256,\n",
    "    stop_token_ids=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/georgo_ho_clues_sampled.csv\")\n",
    "\n",
    "dataset = dataset.sample(frac=0.015).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                       | 0/15 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LLM' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m ans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# correct, response = eval_llama(p, clue, definition_label, model, tokenizer)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m correct, response \u001b[38;5;241m=\u001b[39m \u001b[43meval_llama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROMPTS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDEFINITION_PROMPT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefinition_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(clue)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(definition_label)\n",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m, in \u001b[0;36meval_llama\u001b[0;34m(prompt, clue, target, model, tokenizer, ans, definition)\u001b[0m\n\u001b[1;32m      4\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt\u001b[38;5;241m.\u001b[39mformat(clue\u001b[38;5;241m=\u001b[39mclue, ans\u001b[38;5;241m=\u001b[39mans, definition\u001b[38;5;241m=\u001b[39mdefinition)},\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      7\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m      8\u001b[0m         messages, \n\u001b[1;32m      9\u001b[0m         tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m     10\u001b[0m         add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mllama3_inference\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# response = model.generate([prompt], sampling_params, use_tqdm= False)[0].outputs[0].text.strip().lower()\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# print(response)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/mbzuai/decrypting-crosswords/inference.py:17\u001b[0m, in \u001b[0;36mllama3_inference\u001b[0;34m(model, tokenizer, data, do_sample, temp, max_new_tokens, top_p)\u001b[0m\n\u001b[1;32m     10\u001b[0m     example \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m     11\u001b[0m                     [example], \n\u001b[1;32m     12\u001b[0m                     tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m     13\u001b[0m                     add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     14\u001b[0m             )\n\u001b[1;32m     15\u001b[0m     batch_after_temp\u001b[38;5;241m.\u001b[39mappend(example)\n\u001b[0;32m---> 17\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(batch_after_temp,return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m)\n\u001b[1;32m     19\u001b[0m terminators \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     20\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m     21\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|eot_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     22\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<|end_of_text|>\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     25\u001b[0m inputs_length \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LLM' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "p = \"\"\"You are a cryptic crossword expert. I will give you a cryptic clue. Every clue has two parts: a definition and a wordplay. The definition is a synonym of the clue's answer. Your task is to identify and extract the word/s in the clue. Only output the definition.\n",
    "Clue: {clue}\n",
    "Definition:\n",
    "\"\"\"\n",
    "\n",
    "for i, row in tqdm(dataset.iterrows(),total=dataset.shape[0]):\n",
    "\n",
    "    definition_label = str(row['Definition']).lower()\n",
    "    if '/' in definition_label:\n",
    "        definition_label = definition_label.split('/')\n",
    "    else:\n",
    "        definition_label = [definition_label]\n",
    "\n",
    "    for d in definition_label:\n",
    "        d = d.strip()\n",
    "    clue = str(row['Clue'])\n",
    "    ans = str(row['Answer'])\n",
    "    # correct, response = eval_llama(p, clue, definition_label, model, tokenizer)\n",
    "    correct, response = eval_llama(PROMPTS['DEFINITION_PROMPT'], clue, definition_label, model, tokenizer)\n",
    "    \n",
    "\n",
    "    print(clue)\n",
    "    print(definition_label)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.output_file, 'w') as f:\n",
    "    f.write(f'Evaluation of {args.model}\\n\\n')\n",
    "\n",
    "    definition = PROMPTS[args.prompt]\n",
    "    wordplay = PROMPTS[args.prompt]\n",
    "    f.write(f'Prompts: \\n Definition_prompt: {definition} \\n Wordplay_prompt:{wordplay  }\\n\\n')\n",
    "    f.write(f'Definition Accuracy: {definition_acc/len(dataset)}\\n')\n",
    "    f.write(f'Wordplay Accuracy: {wordplay_acc/len(dataset)}\\n')\n",
    "    f.write('\\n\\n')\n",
    "    f.write('Definition Responses\\n\\n')\n",
    "    for res in definition_responses:\n",
    "        f.write(res)\n",
    "    f.write('\\n\\n')\n",
    "    f.write('Wordplay Responses\\n\\n')\n",
    "    for res in wordplay_responses:\n",
    "        f.write(res)\n",
    "    f.write('\\n\\n') \n",
    "    f.write(f'Total Clues: {len(dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
