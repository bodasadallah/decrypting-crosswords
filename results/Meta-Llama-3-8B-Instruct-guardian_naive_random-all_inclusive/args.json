{"model_args": {"model_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct", "tokenizer_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct", "wandb_project": "cryptic_crosswords", "wandb_run_name": "llama3-base-prompt-guardian_naive_random-0-shot", "checkpoint_path": "", "use_flash_attention_2": true, "do_sample": false, "temperature": 0.6, "max_new_tokens": 1024, "top_p": 0.9}, "data_args": {"dataset": "boda/guardian_naive_random", "split": "test", "n_shots": 0, "results_save_file": "./results/result.txt", "write_outputs_in_results": true, "prompt_key": "prompt", "prompt_head": "ALL_INCLUSIVE_PROMPT", "save_model_predicitons": "yes", "num_examples": 0, "save_folder": "results/Meta-Llama-3-8B-Instruct-guardian_naive_random-all_inclusive"}, "quatization_args": {"quantize": false, "bnb_4bit_quant_type": "nf4", "bnb_4bit_compute_dtype": "bfloat16", "bnb_4bit_use_double_quant": true, "lora_alpha": 16, "lora_dropout": 0.1, "lora_r": 64, "bias": "none", "task_type": "CAUSAL_LM", "lora_target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "load_in_4bit": false}}